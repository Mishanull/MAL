{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Group Assignment\n",
    "- Analyzing news articles to determine if they are fake or real, using a dataset from kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "2. Define the objective in business terms: Create a model capable of recognizing fake and real news.\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?: Supervised learning, with binary labeling (1 for real, 0 for fake news) \n",
    "4. How should performance be measured?: Performance will be measured by the model accuracy, correctly classified vs. all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T20:32:32.332364Z",
     "iopub.status.busy": "2023-11-20T20:32:32.332053Z",
     "iopub.status.idle": "2023-11-20T20:32:32.336389Z",
     "shell.execute_reply": "2023-11-20T20:32:32.335681Z",
     "shell.execute_reply.started": "2023-11-20T20:32:32.332347Z"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "Data were retrieved from Kaggle (link below). This data set consists of 2 csv files, one with the real and other with the real news. Both data sets have 'Title', 'Text', 'Subject' and 'Date' columns. \n",
    "\n",
    "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "In the next step we labeled fake and real data, with 0 and 1 respectively and then merged them into 1 dataframe and shuffled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:25.158433Z",
     "iopub.status.busy": "2023-11-20T19:10:25.158230Z",
     "iopub.status.idle": "2023-11-20T19:10:25.714629Z",
     "shell.execute_reply": "2023-11-20T19:10:25.714110Z",
     "shell.execute_reply.started": "2023-11-20T19:10:25.158417Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load & label fake news data\n",
    "df_fake = pd.read_csv('Fake.csv')\n",
    "df_fake['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:25.715525Z",
     "iopub.status.busy": "2023-11-20T19:10:25.715374Z",
     "iopub.status.idle": "2023-11-20T19:10:26.219823Z",
     "shell.execute_reply": "2023-11-20T19:10:26.219284Z",
     "shell.execute_reply.started": "2023-11-20T19:10:25.715511Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load & label real news data\n",
    "df_true = pd.read_csv('True.csv')\n",
    "df_true['label'] = 1\n",
    "#rename all 'politicsNews' to 'politics' in df_true\n",
    "df_true['subject'] = df_true['subject'].replace('politicsNews', 'politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:26.220786Z",
     "iopub.status.busy": "2023-11-20T19:10:26.220601Z",
     "iopub.status.idle": "2023-11-20T19:10:26.227819Z",
     "shell.execute_reply": "2023-11-20T19:10:26.227049Z",
     "shell.execute_reply.started": "2023-11-20T19:10:26.220761Z"
    }
   },
   "outputs": [],
   "source": [
    "#print distinct subjects\n",
    "print(df_fake['subject'].unique())\n",
    "print(df_true['subject'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:49.606012Z",
     "iopub.status.busy": "2023-11-20T19:10:49.605634Z",
     "iopub.status.idle": "2023-11-20T19:10:49.639645Z",
     "shell.execute_reply": "2023-11-20T19:10:49.638605Z",
     "shell.execute_reply.started": "2023-11-20T19:10:49.605995Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge & shuffle data\n",
    "df_news = pd.concat([df_fake, df_true]).sample(frac=1).reset_index(drop=True)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore and visualise the data to gain insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:52.261435Z",
     "iopub.status.busy": "2023-11-20T19:10:52.261015Z",
     "iopub.status.idle": "2023-11-20T19:10:52.271262Z",
     "shell.execute_reply": "2023-11-20T19:10:52.270706Z",
     "shell.execute_reply.started": "2023-11-20T19:10:52.261414Z"
    }
   },
   "outputs": [],
   "source": [
    "#group data by subject and label\n",
    "df_news.groupby(['subject', 'label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:53.183815Z",
     "iopub.status.busy": "2023-11-20T19:10:53.183398Z",
     "iopub.status.idle": "2023-11-20T19:10:53.665271Z",
     "shell.execute_reply": "2023-11-20T19:10:53.664763Z",
     "shell.execute_reply.started": "2023-11-20T19:10:53.183796Z"
    }
   },
   "outputs": [],
   "source": [
    "#fake vs real news\n",
    "print(df_news.groupby(['label']).size())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.bar(df_news['label'].unique(), df_news.groupby(['label']).size(), color=['#e06666','#93c47d'])\n",
    "plt.xticks(df_news['label'].unique(), ['Fake', 'Real'])\n",
    "plt.xlabel('News')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Fake vs Real News')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:54.231114Z",
     "iopub.status.busy": "2023-11-20T19:10:54.230516Z",
     "iopub.status.idle": "2023-11-20T19:10:54.391859Z",
     "shell.execute_reply": "2023-11-20T19:10:54.391399Z",
     "shell.execute_reply.started": "2023-11-20T19:10:54.231087Z"
    }
   },
   "outputs": [],
   "source": [
    "#number of articles per subject and label\n",
    "df_news.groupby(['subject', 'label']).size().unstack().plot(kind='bar', color=['#e06666','#93c47d'], stacked=False, figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:10:54.707315Z",
     "iopub.status.busy": "2023-11-20T19:10:54.706856Z",
     "iopub.status.idle": "2023-11-20T19:10:54.736773Z",
     "shell.execute_reply": "2023-11-20T19:10:54.736094Z",
     "shell.execute_reply.started": "2023-11-20T19:10:54.707293Z"
    }
   },
   "outputs": [],
   "source": [
    "#print the full text of the first 5 articles of subject 'worldnews'\n",
    "for i in range(5):\n",
    "    print(df_news[(df_news['subject'] == 'worldnews') & (df_news['label'] == 1)]['text'].iloc[i])\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:11:10.537868Z",
     "iopub.status.busy": "2023-11-20T19:11:10.537645Z",
     "iopub.status.idle": "2023-11-20T19:11:12.745219Z",
     "shell.execute_reply": "2023-11-20T19:11:12.744711Z",
     "shell.execute_reply.started": "2023-11-20T19:11:10.537851Z"
    }
   },
   "outputs": [],
   "source": [
    "#average word count of articles per label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(df_news.groupby(['label'])['text'].apply(lambda x: x.str.split().str.len().mean()))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.bar(df_news['label'].unique(), df_news.groupby(['label'])['text'].apply(lambda x: x.str.split().str.len().mean()), color=['#e06666','#93c47d'])\n",
    "plt.title('Average Word Count per Label')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.xlabel('Label')\n",
    "plt.xticks(np.arange(2), ('Fake', 'Real'), rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:11:14.516042Z",
     "iopub.status.busy": "2023-11-20T19:11:14.515828Z",
     "iopub.status.idle": "2023-11-20T19:11:16.347295Z",
     "shell.execute_reply": "2023-11-20T19:11:16.346882Z",
     "shell.execute_reply.started": "2023-11-20T19:11:14.516022Z"
    }
   },
   "outputs": [],
   "source": [
    "#average word count of articles per subject per label\n",
    "df_news.groupby(['subject', 'label'])['text'].apply(lambda x: x.str.split().str.len().mean()).unstack().plot(kind='bar', color=['#e06666','#93c47d'], figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:11:18.513916Z",
     "iopub.status.busy": "2023-11-20T19:11:18.513700Z",
     "iopub.status.idle": "2023-11-20T19:11:18.676541Z",
     "shell.execute_reply": "2023-11-20T19:11:18.676035Z",
     "shell.execute_reply.started": "2023-11-20T19:11:18.513900Z"
    }
   },
   "outputs": [],
   "source": [
    "#correlation between date and label\n",
    "df_news.groupby(['date', 'label']).size().unstack().plot(kind='line', color=['#e06666','#93c47d'], figsize=(15, 5), label=['Fake', 'Real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T19:11:18.844007Z",
     "iopub.status.busy": "2023-11-20T19:11:18.843801Z",
     "iopub.status.idle": "2023-11-20T19:11:19.018505Z",
     "shell.execute_reply": "2023-11-20T19:11:19.017971Z",
     "shell.execute_reply.started": "2023-11-20T19:11:18.843991Z"
    }
   },
   "outputs": [],
   "source": [
    "#add column 'contains_Reuters' that will be 1 if the article contains the word 'Reuters' and 0 otherwise\n",
    "df_news['contains_Reuters'] = df_news['text'].apply(lambda x: 1 if 'Reuters' in x else 0)\n",
    "\n",
    "#correlation between label and 'contains_Reuters'\n",
    "df_news.groupby(['contains_Reuters', 'label']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    color=['#e06666','#93c47d'],\n",
    "    title=\"#Labels if contains 'Reuters'\",\n",
    "    figsize=(5, 5),\n",
    "    ).set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "\n",
    "#remove contains_Reuters column\n",
    "df_news.drop(['contains_Reuters'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The plot above shows the correlation between the authenticity of the news and whether the news agency Reuters appears as a source or not, which we noticed while taking a first glance at the data.\n",
    "- This is self-explanatory, since Reuters is a reliable source of information.\n",
    "- There are a few outliers where the news are fake even if 'Reuters' appears in the text or is cited as the source, but the text overall probably contains half true information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many articles contain the word 'Reuters' in the title\n",
    "print(df_news['title'].apply(lambda x: 1 if 'Reuters' in x else 0).sum())\n",
    "\n",
    "#count how many articles contain the word 'Reuters' in the text\n",
    "print(df_news['text'].apply(lambda x: 1 if 'Reuters' in x else 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove REUTERS, Reuters, reuters from text\n",
    "df_news['text'] = df_news['text'].apply(lambda x: x.replace('REUTERS', ''))\n",
    "df_news['text'] = df_news['text'].apply(lambda x: x.replace('Reuters', ''))\n",
    "df_news['text'] = df_news['text'].apply(lambda x: x.replace('reuters', ''))\n",
    "\n",
    "#remove REUTERS, Reuters, reuters from title\n",
    "df_news['title'] = df_news['title'].apply(lambda x: x.replace('REUTERS', ''))\n",
    "df_news['title'] = df_news['title'].apply(lambda x: x.replace('Reuters', ''))\n",
    "df_news['title'] = df_news['title'].apply(lambda x: x.replace('reuters', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- function for transforming text data: TF-IDF (Term Frequency - Inverse Document Frequency) \n",
    "- splitting the data including training, testing and validation sets\n",
    "- using only title and text on the X variable\n",
    "- labels will be the label column (0 or 1, namely fake or real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T20:44:11.082081Z",
     "iopub.status.busy": "2023-11-20T20:44:11.081663Z",
     "iopub.status.idle": "2023-11-20T20:44:11.087515Z",
     "shell.execute_reply": "2023-11-20T20:44:11.086937Z",
     "shell.execute_reply.started": "2023-11-20T20:44:11.082055Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def transform_with_tfidf(X_train, X_val, X_test, max_df=0.6, max_features=None):\n",
    "    \"\"\"\n",
    "    Transform input data using TfidfVectorizer.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training data\n",
    "    - X_val: Validation data\n",
    "    - X_test: Test data\n",
    "    - max_df: Maximum document frequency for TfidfVectorizer\n",
    "\n",
    "    Returns:\n",
    "    - X_train_tfidf: Transformed training data\n",
    "    - X_val_tfidf: Transformed validation data\n",
    "    - X_test_tfidf: Transformed test data\n",
    "    \"\"\"\n",
    "\n",
    "    if max_features:\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=max_df, max_features=max_features)\n",
    "    else:\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=max_df)\n",
    "\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T20:21:07.131920Z",
     "iopub.status.busy": "2023-11-20T20:21:07.131699Z",
     "iopub.status.idle": "2023-11-20T20:21:07.240468Z",
     "shell.execute_reply": "2023-11-20T20:21:07.239922Z",
     "shell.execute_reply.started": "2023-11-20T20:21:07.131900Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_news['title'] + ' ' + df_news['text']\n",
    "y = df_news['label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Short-list promising models\n",
    "- we are sorting them by the method we are using to transform the text:\n",
    "   - for TF-IDF: MultinomialNaiveBayes, K-Nearest-Neighbors as classifiers, CNN and RNN as neural networks\n",
    "   - for Word2Vec: A simple neural network with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T20:21:10.789224Z",
     "iopub.status.busy": "2023-11-20T20:21:10.788271Z",
     "iopub.status.idle": "2023-11-20T20:21:20.518650Z",
     "shell.execute_reply": "2023-11-20T20:21:20.518126Z",
     "shell.execute_reply.started": "2023-11-20T20:21:10.789162Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = transform_with_tfidf(X_train, X_val, X_test, max_df=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Multinomial naive bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T20:30:36.940812Z",
     "iopub.status.busy": "2023-11-20T20:30:36.940487Z",
     "iopub.status.idle": "2023-11-20T20:30:37.013270Z",
     "shell.execute_reply": "2023-11-20T20:30:37.012641Z",
     "shell.execute_reply.started": "2023-11-20T20:30:36.940795Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "y_val_pred = clf.predict(X_val_tfidf)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) KNN with an arbitrary number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T20:49:01.276682Z",
     "iopub.status.busy": "2023-11-20T20:49:01.276491Z",
     "iopub.status.idle": "2023-11-20T20:56:48.687149Z",
     "shell.execute_reply": "2023-11-20T20:56:48.686628Z",
     "shell.execute_reply.started": "2023-11-20T20:49:01.276665Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5) \n",
    "knn_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = knn_classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "from tensorflow import keras\n",
    "\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = transform_with_tfidf(X_train, X_val, X_test, max_df=0.6, max_features=5000)\n",
    "\n",
    "cnn_model = keras.Sequential([\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(X_train_tfidf.shape[1], 1)),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print type of X_train_tfidf\n",
    "print(type(X_train_tfidf))\n",
    "\n",
    "#count number of vectors in first row of X_train_tfidf\n",
    "print(len(X_train_tfidf[0].data))\n",
    "print(len(X_train_tfidf[1].data))\n",
    "print(len(X_train_tfidf[2].data))\n",
    "print(len(X_train_tfidf[-2].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf[-2].shape)\n",
    "print(X_train_tfidf[-2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "X_train_tfidf_arr = X_train_tfidf.toarray()\n",
    "X_val_tfidf_arr = X_val_tfidf.toarray()\n",
    "\n",
    "history = cnn_model.fit(X_train_tfidf_arr, y_train, epochs=10, batch_size=128, validation_data=(X_val_tfidf_arr, y_val), callbacks=[es])\n",
    "\n",
    "#plot training and validation accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('CNN Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('CNN Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model on test data\n",
    "X_test_tfidf_arr = X_test_tfidf.toarray()\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(X_test_tfidf_arr, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = transform_with_tfidf(X_train, X_val, X_test, max_df=0.6, max_features=10000)\n",
    "\n",
    "# lr = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=42, C=0.5, max_iter=1000)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print one of the misclassified articles\n",
    "print(X_test[y_test != y_pred].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Recursive Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM RNN\n",
    "\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = transform_with_tfidf(X_train, X_val, X_test, max_df=0.6, max_features=5000)\n",
    "\n",
    "lstm_model = keras.Sequential([\n",
    "    keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(X_train_tfidf.shape[1], 1)),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "X_train_tfidf_arr = X_train_tfidf.toarray()\n",
    "X_val_tfidf_arr = X_val_tfidf.toarray()\n",
    "\n",
    "history = lstm_model.fit(X_train_tfidf_arr, y_train, epochs=10, batch_size=128, validation_data=(X_val_tfidf_arr, y_val), callbacks=[es])\n",
    "\n",
    "#plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('LSTM Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('LSTM Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model on test data\n",
    "X_test_tfidf_arr = X_test_tfidf.toarray()\n",
    "\n",
    "test_loss, test_acc = lstm_model.evaluate(X_test_tfidf_arr, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Testing out the most promising one from above with new data (some random article texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Using WordEmbeddings for text representation (Word2Vec) and a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:45:02.874471Z",
     "iopub.status.busy": "2023-11-20T12:45:02.873519Z",
     "iopub.status.idle": "2023-11-20T12:58:04.146734Z",
     "shell.execute_reply": "2023-11-20T12:58:04.146100Z",
     "shell.execute_reply.started": "2023-11-20T12:45:02.874447Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_news['title'] + ' ' + df_news['text']\n",
    "y = df_news['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 10000  # Adjust based on the size of your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure consistent length for input to neural network\n",
    "max_sequence_length = 1000  # Adjust based on your dataset\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "# Train Word2Vec model\n",
    "tokenized_sentences = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        try:\n",
    "            embedding_vector = word2vec_model.wv[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            # Word not in Word2Vec model vocabulary\n",
    "            pass\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:58:21.759672Z",
     "iopub.status.busy": "2023-11-20T12:58:21.759449Z",
     "iopub.status.idle": "2023-11-20T12:58:26.383241Z",
     "shell.execute_reply": "2023-11-20T12:58:26.382688Z",
     "shell.execute_reply.started": "2023-11-20T12:58:21.759658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(((y_pred > 0.5).astype(int)))\n",
    "print(y_pred_labels)\n",
    "print(y_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print classification report for the test set\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Choosing a model and fine-tuning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Video link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
