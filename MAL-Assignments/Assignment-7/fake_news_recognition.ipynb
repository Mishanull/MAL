{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Group Assignment\n",
    "- Analyzing news articles to determine if they are fake or real, based on a dataset from kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "2. Define the objective in business terms: Create a model capable of recognizing fake and real news.\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?: Supervised learning, with binary labeling (1 for real, 0 for fake news) \n",
    "4. How should performance be measured?: Performance will be measured by the model accuracy, correctly classified vs. all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:20:03.154776Z",
     "iopub.status.busy": "2023-11-20T12:20:03.154561Z",
     "iopub.status.idle": "2023-11-20T12:20:03.190522Z",
     "shell.execute_reply": "2023-11-20T12:20:03.189904Z",
     "shell.execute_reply.started": "2023-11-20T12:20:03.154755Z"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "Data were retrieved from Kaggle (link below). This data set consists of 2 csv files, one with the real and other with the real news. Both data sets have 'Title', 'Text', 'Subject' and 'Date' columns. \n",
    "\n",
    "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "In the next step we labeled fake and real data, with 0 and 1 respectively and then merged them into 1 dataframe and shuffled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:04:55.507180Z",
     "iopub.status.busy": "2023-11-20T12:04:55.506996Z",
     "iopub.status.idle": "2023-11-20T12:04:56.075466Z",
     "shell.execute_reply": "2023-11-20T12:04:56.074928Z",
     "shell.execute_reply.started": "2023-11-20T12:04:55.507165Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load & label fake news data\n",
    "df_fake = pd.read_csv('Fake.csv')\n",
    "df_fake['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:04:56.076394Z",
     "iopub.status.busy": "2023-11-20T12:04:56.076237Z",
     "iopub.status.idle": "2023-11-20T12:04:56.565789Z",
     "shell.execute_reply": "2023-11-20T12:04:56.565323Z",
     "shell.execute_reply.started": "2023-11-20T12:04:56.076379Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load & label real news data\n",
    "df_true = pd.read_csv('True.csv')\n",
    "df_true['label'] = 1\n",
    "\n",
    "#rename all 'politicsNews' to 'politics' in df_true\n",
    "df_true['subject'] = df_true['subject'].replace('politicsNews', 'politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print distinct subjects\n",
    "print(df_fake.subject.unique())\n",
    "print(df_true.subject.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:04:56.566635Z",
     "iopub.status.busy": "2023-11-20T12:04:56.566484Z",
     "iopub.status.idle": "2023-11-20T12:04:56.591538Z",
     "shell.execute_reply": "2023-11-20T12:04:56.591009Z",
     "shell.execute_reply.started": "2023-11-20T12:04:56.566621Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge & shuffle data\n",
    "df_news = pd.concat([df_fake, df_true]).sample(frac=1).reset_index(drop=True)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group data by subject and label\n",
    "df_news.groupby(['subject', 'label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake vs real news\n",
    "df_news.groupby(['label']).size().plot(kind='bar', color=['red', 'blue']).set_xticklabels(['Fake', 'Real'], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of articles per subject and label\n",
    "df_news.groupby(['subject', 'label']).size().unstack().plot(kind='bar', stacked=False, figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the full text of the first 5 articles of subject 'worldnews'\n",
    "for i in range(5):\n",
    "    print(df_news[(df_news['subject'] == 'worldnews') & (df_news['label'] == 1)]['text'].iloc[i])\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average word count of articles per label\n",
    "df_news.groupby(['label'])['text'].apply(lambda x: x.str.split().str.len().mean()).plot(kind='bar', color=['red', 'blue']).set_xticklabels(['Fake', 'Real'], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average word count of articles per subject per label\n",
    "df_news.groupby(['subject', 'label'])['text'].apply(lambda x: x.str.split().str.len().mean()).unstack().plot(kind='bar', figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "correlation matrix: Title-subject-Label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between date and label\n",
    "df_news.groupby(['date', 'label']).size().unstack().plot(kind='line', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column 'contains_Reuters' that will be 1 if the article contains the word 'Reuters' and 0 otherwise\n",
    "df_news['contains_Reuters'] = df_news['text'].apply(lambda x: 1 if 'Reuters' in x else 0)\n",
    "\n",
    "#correlation between label and 'contains_Reuters'\n",
    "df_news.groupby(['contains_Reuters', 'label']).size().unstack().plot(kind='bar', color=['red', 'blue']).set_xticklabels(['No', 'Yes'], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:04:56.650758Z",
     "iopub.status.busy": "2023-11-20T12:04:56.650494Z",
     "iopub.status.idle": "2023-11-20T12:04:56.659886Z",
     "shell.execute_reply": "2023-11-20T12:04:56.659252Z",
     "shell.execute_reply.started": "2023-11-20T12:04:56.650739Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping date and subject\n",
    "df_news = df_news.drop(columns = ['subject','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:04:57.059564Z",
     "iopub.status.busy": "2023-11-20T12:04:57.059378Z",
     "iopub.status.idle": "2023-11-20T12:04:57.066101Z",
     "shell.execute_reply": "2023-11-20T12:04:57.065544Z",
     "shell.execute_reply.started": "2023-11-20T12:04:57.059549Z"
    }
   },
   "outputs": [],
   "source": [
    "df_news.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Splitting the data and vectorizing using TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "- additionaly, we are testing it out with a Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Splitting the data, while including the title in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:04:58.943392Z",
     "iopub.status.busy": "2023-11-20T12:04:58.943071Z",
     "iopub.status.idle": "2023-11-20T12:04:59.058725Z",
     "shell.execute_reply": "2023-11-20T12:04:59.058168Z",
     "shell.execute_reply.started": "2023-11-20T12:04:58.943376Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_news['title'] + ' ' + df_news['text']\n",
    "y = df_news['label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:06:14.891921Z",
     "iopub.status.busy": "2023-11-20T12:06:14.891707Z",
     "iopub.status.idle": "2023-11-20T12:06:24.217608Z",
     "shell.execute_reply": "2023-11-20T12:06:24.217061Z",
     "shell.execute_reply.started": "2023-11-20T12:06:14.891905Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.6)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same vectorizer\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:07:57.224578Z",
     "iopub.status.busy": "2023-11-20T12:07:57.224028Z",
     "iopub.status.idle": "2023-11-20T12:07:57.227393Z",
     "shell.execute_reply": "2023-11-20T12:07:57.226629Z",
     "shell.execute_reply.started": "2023-11-20T12:07:57.224559Z"
    }
   },
   "source": [
    "## b) Using Grid Search to perform 5 fold cross-validation in order to get the best params and estimator for MultinomialNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:06:24.218609Z",
     "iopub.status.busy": "2023-11-20T12:06:24.218431Z",
     "iopub.status.idle": "2023-11-20T12:06:25.665091Z",
     "shell.execute_reply": "2023-11-20T12:06:25.664471Z",
     "shell.execute_reply.started": "2023-11-20T12:06:24.218596Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}\n",
    "nb_classifier = MultinomialNB()\n",
    "grid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "y_val_pred = best_classifier.predict(X_val_tfidf)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}')\n",
    "\n",
    "y_test_pred = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:09:03.944444Z",
     "iopub.status.busy": "2023-11-20T12:09:03.944246Z",
     "iopub.status.idle": "2023-11-20T12:09:03.947139Z",
     "shell.execute_reply": "2023-11-20T12:09:03.946560Z",
     "shell.execute_reply.started": "2023-11-20T12:09:03.944430Z"
    }
   },
   "source": [
    "### Ultimately, it seems like MNB yielded impressively good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) We do the same using a CNN to investigate whether it will output better results than the MNB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Using the same vectorizer, we try it out with RNN to analyze if this will yield more accuracy overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Using WordEmbeddings for text representation (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:45:02.874471Z",
     "iopub.status.busy": "2023-11-20T12:45:02.873519Z",
     "iopub.status.idle": "2023-11-20T12:58:04.146734Z",
     "shell.execute_reply": "2023-11-20T12:58:04.146100Z",
     "shell.execute_reply.started": "2023-11-20T12:45:02.874447Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_news['title'] + ' ' + df_news['text']\n",
    "y = df_news['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 10000  # Adjust based on the size of your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure consistent length for input to neural network\n",
    "max_sequence_length = 1000  # Adjust based on your dataset\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "# Train Word2Vec model\n",
    "tokenized_sentences = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        try:\n",
    "            embedding_vector = word2vec_model.wv[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            # Word not in Word2Vec model vocabulary\n",
    "            pass\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T12:58:21.759672Z",
     "iopub.status.busy": "2023-11-20T12:58:21.759449Z",
     "iopub.status.idle": "2023-11-20T12:58:26.383241Z",
     "shell.execute_reply": "2023-11-20T12:58:26.382688Z",
     "shell.execute_reply.started": "2023-11-20T12:58:21.759658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(((y_pred > 0.5).astype(int)))\n",
    "print(y_pred_labels)\n",
    "print(y_test)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print classification report for the test set\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
