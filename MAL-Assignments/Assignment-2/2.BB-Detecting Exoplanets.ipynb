{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting ExoPlanets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Task\n",
    "- Exploring data set\n",
    "- Finding missing values\n",
    "- Finding outliers and determining what to do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (change this if you want other var-names, etc.)\n",
    "import pandas as pd\n",
    "\n",
    "exoplanet_df = pd.read_csv('exoplanet_dataset.csv')\n",
    "\n",
    "print(exoplanet_df.shape, \"- 9564 rows with 49 features\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "exoplanet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an easier comprehension, we will rename the columns into their description.\n",
    "\n",
    "exoplanet_df = exoplanet_df.rename(columns={'kepid':'KepID',\n",
    "'kepoi_name':'KOIName',\n",
    "'kepler_name':'KeplerName',\n",
    "'koi_disposition':'ExoplanetArchiveDisposition',\n",
    "'koi_pdisposition':'DispositionUsingKeplerData',\n",
    "'koi_score':'DispositionScore',\n",
    "'koi_fpflag_nt':'NotTransit-LikeFalsePositiveFlag',\n",
    "'koi_fpflag_ss':'koi_fpflag_ss',\n",
    "'koi_fpflag_co':'CentroidOffsetFalsePositiveFlag',\n",
    "'koi_fpflag_ec':'EphemerisMatchIndicatesContaminationFalsePositiveFlag',\n",
    "'koi_period':'OrbitalPeriod, days',\n",
    "'koi_period_err1':'OrbitalPeriodUpperUnc, days',\n",
    "'koi_period_err2':'OrbitalPeriodLowerUnc, days',\n",
    "'koi_time0bk':'TransitEpoch, BKJD',\n",
    "'koi_time0bk_err1':'TransitEpochUpperUnc, BKJD',\n",
    "'koi_time0bk_err2':'TransitEpochLowerUnc, BKJD',\n",
    "'koi_impact':'ImpactParamete',\n",
    "'koi_impact_err1':'ImpactParameterUpperUnc',\n",
    "'koi_impact_err2':'ImpactParameterLowerUnc',\n",
    "'koi_duration':'TransitDuration, hrs',\n",
    "'koi_duration_err1':'TransitDurationUpperUnc, hrs',\n",
    "'koi_duration_err2':'TransitDurationLowerUnc, hrs',\n",
    "'koi_depth':'TransitDepth, ppm',\n",
    "'koi_insol':'InsolationFlux, Earthflux',\n",
    "'koi_insol_err1':'InsolationFluxUpperUnc, Earthflux',\n",
    "'koi_insol_err2':'InsolationFluxLowerUnc, Earthflux',\n",
    "'koi_model_snr':'TransitSignal-to-Noise',\n",
    "'koi_tce_plnt_num':'TCEPlanetNumber',\n",
    "'koi_tce_delivname':'TCEDeliver',\n",
    "'koi_steff':'StellarEffectiveTemperature, K',\n",
    "'koi_steff_err1':'StellarEffectiveTemperatureUpperUnc, K',\n",
    "'koi_steff_err2':'StellarEffectiveTemperatureLowerUnc, K',\n",
    "'koi_depth_err1':'TransitDepthUpperUnc, ppm',\n",
    "'koi_depth_err2':'TransitDepthLowerUnc, ppm',\n",
    "'koi_prad':'PlanetaryRadius, Earthradii',\n",
    "'koi_prad_err1':'PlanetaryRadiusUpperUnc, Earthradii',\n",
    "'koi_prad_err2':'PlanetaryRadiusLowerUnc, Earthradii',\n",
    "'koi_teq':'EquilibriumTemperature, K',\n",
    "'koi_teq_err1':'EquilibriumTemperatureUpperUnc, K',\n",
    "'koi_teq_err2':'EquilibriumTemperatureLowerUnc, K',\n",
    "'koi_slogg':'StellarSurfaceGravity, log10(cm/s^2)',\n",
    "'koi_slogg_err1':'StellarSurfaceGravityUpperUnc, log10(cm/s^2)',\n",
    "'koi_slogg_err2':'StellarSurfaceGravityLowerUnc, log10(cm/s^2)',\n",
    "'koi_srad':'StellarRadius, Solarradii',\n",
    "'koi_srad_err1':'StellarRadiusUpperUnc, Solarradii',\n",
    "'koi_srad_err2':'StellarRadiusLowerUnc, Solarradii',\n",
    "'ra':'RA, decimaldegrees',\n",
    "'dec':'Dec, decimaldegrees',\n",
    "'koi_kepmag':'Kepler-band, mag'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print distinct values of DispositionUsingKeplerData and ExoplanetArchiveDisposition\n",
    "print(exoplanet_df['DispositionUsingKeplerData'].unique())\n",
    "print(exoplanet_df['ExoplanetArchiveDisposition'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x = exoplanet_df['DispositionUsingKeplerData'])\n",
    "print(exoplanet_df['DispositionUsingKeplerData'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x= exoplanet_df['ExoplanetArchiveDisposition'])\n",
    "print(exoplanet_df['ExoplanetArchiveDisposition'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find percentege of missing values for each column, print as dataframe\n",
    "missing_values = exoplanet_df.isnull().sum().sort_values(ascending=False)\n",
    "percentage_missing_values = ((missing_values/len(exoplanet_df))*100).round(2)\n",
    "percentage_missing_values = percentage_missing_values.to_frame()\n",
    "percentage_missing_values.columns = ['Percentage of missing values']\n",
    "percentage_missing_values\n",
    "\n",
    "#visualize 5 columns with most missing values\n",
    "from matplotlib import pyplot as plt\n",
    "sns.barplot(x=percentage_missing_values.index[0:5], y='Percentage of missing values', data=percentage_missing_values[0:5])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify potential outliers of numerical columns\n",
    "numerical_columns = exoplanet_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_columns = numerical_columns.drop(['KepID'])\n",
    "\n",
    "#iterate through numerical columns and calculate number of values Q1-1.5*IQR and Q3+1.5*IQR\n",
    "for column in numerical_columns:\n",
    "    q1 = exoplanet_df[column].quantile(0.25)\n",
    "    q3 = exoplanet_df[column].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    lower_limit = q1-1.5*iqr\n",
    "    upper_limit = q3+1.5*iqr\n",
    "    print(column, \":\", exoplanet_df[(exoplanet_df[column]<lower_limit) | (exoplanet_df[column]>upper_limit)][column].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with 100% missing values\n",
    "exoplanet_df = exoplanet_df.drop(['EquilibriumTemperatureLowerUnc, K', 'EquilibriumTemperatureUpperUnc, K'], axis=1)\n",
    "#remove irrelevant columns (names, ids, etc.)\n",
    "exoplanet_df = exoplanet_df.drop(['KepID', 'KOIName', 'KeplerName', 'TCEPlanetNumber', 'TCEDeliver'], axis=1)\n",
    "#remove NaN values\n",
    "exoplanet_df = exoplanet_df.dropna()\n",
    "\n",
    "#replace outliers with median\n",
    "# numerical_columns = exoplanet_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "# for column in numerical_columns:\n",
    "#     q1 = exoplanet_df[column].quantile(0.25)\n",
    "#     q3 = exoplanet_df[column].quantile(0.75)\n",
    "#     iqr = q3-q1\n",
    "#     lower_limit = q1-1.5*iqr\n",
    "#     upper_limit = q3+1.5*iqr\n",
    "#     exoplanet_df[column] = exoplanet_df[column].mask((exoplanet_df[column]<lower_limit) | (exoplanet_df[column]>upper_limit), exoplanet_df[column].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Our choice was to keep outliers in case they represent useful values in the data or anomalies that may prove to be useful in predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature engineering\n",
    "- Removing columns with 100% missing values\n",
    "- Removing irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns based on 'DispositionUsingKeplerData' and 'ExoplanetArchiveDisposition' columns named 'KeplerDispositionStatus' and 'ArchiveDispositionStatus'\n",
    "#if 'FALSE POSITIVE' then 0, else if 'Candidate' then 1, else if 'CONFIRMED' then 2\n",
    "exoplanet_df['KeplerDispositionStatus'] = exoplanet_df['DispositionUsingKeplerData'].apply(lambda x: 0 if x == 'FALSE POSITIVE' else (1 if x == 'CANDIDATE' else 2))\n",
    "exoplanet_df['ArchiveDispositionStatus'] = exoplanet_df['ExoplanetArchiveDisposition'].apply(lambda x: 0 if x == 'FALSE POSITIVE' else (1 if x == 'CANDIDATE' else 2))\n",
    "#drop 'DispositionUsingKeplerData' and 'ExoplanetArchiveDisposition' columns\n",
    "exoplanet_df = exoplanet_df.drop(['DispositionUsingKeplerData', 'ExoplanetArchiveDisposition'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find correlation with 'KeplerDispositionStatus' (target) and sort values\n",
    "correlation_with_target = exoplanet_df.corrwith(exoplanet_df['KeplerDispositionStatus']).sort_values(ascending=False)\n",
    "print(correlation_with_target)\n",
    "\n",
    "#store columns with correlation < x in a list for later dropping\n",
    "x = 0.2\n",
    "columns_to_drop = []\n",
    "for i in range(len(correlation_with_target)):\n",
    "    if abs(correlation_with_target.iloc[i]) < x:\n",
    "        # print(correlation_with_target.index[i], correlation_with_target.iloc[i])\n",
    "        columns_to_drop.append(correlation_with_target.index[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with correlation < x\n",
    "# exoplanet_df = exoplanet_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place 'KeplerDispositionStatus' as target column and remove 'DispositionScore', 'KeplerDispositionStatus' and 'ArchiveDispositionStatus' columns\n",
    "target_column = exoplanet_df['KeplerDispositionStatus']\n",
    "model_df = exoplanet_df.drop(['DispositionScore', 'KeplerDispositionStatus', 'ArchiveDispositionStatus'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = model_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "#create a correlation matrix of numerical columns\n",
    "correlation_matrix = model_df[numerical_columns].corr().round(2)\n",
    "\n",
    "#print columns with correlation > x and store them in a list for later dropping\n",
    "x = 0.75\n",
    "columns_to_drop = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > x:\n",
    "            print(correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j])\n",
    "            columns_to_drop.append(correlation_matrix.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with correlation > x\n",
    "model_df = model_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get min & max values and skewness of numerical columns and print as dataframe\n",
    "from scipy.stats import skew\n",
    "numerical_columns = model_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "min_values = model_df[numerical_columns].min()\n",
    "max_values = model_df[numerical_columns].max()\n",
    "skewness = model_df[numerical_columns].skew()\n",
    "skewness = skewness.round(2)\n",
    "skewness = skewness.to_frame()\n",
    "skewness.columns = ['Skewness']\n",
    "\n",
    "val_range = min_values.to_frame()\n",
    "val_range.columns = ['Min']\n",
    "val_range['Max'] = max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "#scale data by dividing by L2 norm\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# normalizer = Normalizer()\n",
    "# model_df = normalizer.fit_transform(model_df)\n",
    "\n",
    "#scale data by dividing by Standard Deviation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "model_df = scaler.fit_transform(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 + 4 Splitting the data and training the model\n",
    "- Splitting into test and train data\n",
    "- fitting & tuning KNN model\n",
    "- Using Validation & Cross Validation to determine the best neighbors hyperparamater for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train, test and validation sets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainVal, X_test, y_trainVal, y_test = train_test_split(model_df, target_column, stratify=target_column, test_size=0.25, random_state=69)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainVal, y_trainVal, test_size=0.25, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "for num_neighbors in range(1,15):\n",
    "    # Learn the model with a certain numnber of neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=num_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    score = knn.score(X_val, y_val)\n",
    "    \n",
    "    # If improvement, store score and parameter\n",
    "    if score>best_score:\n",
    "        best_score = score\n",
    "        best_num_neighbors = num_neighbors\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "knn = KNeighborsClassifier(n_neighbors=best_num_neighbors)\n",
    "knn.fit(X_trainVal, y_trainVal)\n",
    "\n",
    "print(\"Best number of neighbors found: {}\".format(best_num_neighbors))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(knn.score(X_trainVal, y_trainVal)))\n",
    "print(\"Score on test set: {}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting hyperparameters using cross-validation\n",
    "best_num_neighbors = 0\n",
    "best_score = 0\n",
    "for num_neighbors in range(1,15):\n",
    "    # Set a certain number of neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=num_neighbors)\n",
    "    \n",
    "    # Perform cross validation\n",
    "    scores = cross_val_score(knn, X_trainVal, y_trainVal, cv=5)\n",
    "    \n",
    "    # Compute the mean score\n",
    "    score = scores.mean()\n",
    "    print(\"Number of neighbors: {}, score: {}\".format(num_neighbors, score))\n",
    "    \n",
    "    # If improvement, store score and parameter\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_num_neighbors = num_neighbors\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "knn = KNeighborsClassifier(n_neighbors=best_num_neighbors)\n",
    "knn.fit(X_trainVal, y_trainVal)\n",
    "\n",
    "print(\"Best number of neighbors found: {}\".format(best_num_neighbors))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(knn.score(X_trainVal, y_trainVal)))\n",
    "print(\"Score on test set: {}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Writing functions for ROC Curve, Precision-Recall Curve and one for the Confussion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate accuracy, precision, recall and f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#visualize confusion matrix as heatmap\n",
    "heatmap = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "heatmap.set_xlabel('Predicted')\n",
    "heatmap.set_ylabel('True')\n",
    "heatmap.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display precision-recall curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "plt.plot(precision, recall)\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "neighoursRange = range(1,25)\n",
    "for i in neighoursRange:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy.append(knn.score(X_train, y_train))\n",
    "    test_accuracy.append(knn.score(X_test, y_test))\n",
    "\n",
    "plt.plot(neighoursRange, train_accuracy, label='train')\n",
    "plt.plot(neighoursRange, test_accuracy, label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {}\".format(gnb.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {}\".format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {}\".format(linreg.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {}\".format(linreg.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
