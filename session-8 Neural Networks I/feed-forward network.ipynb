{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from plot_utils import plot_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks on non-linear data\n",
    "\n",
    "Some familiar datasets to see how neural networks handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "colors = ListedColormap(['green', 'magenta', 'yellow', 'blue'])\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise = .2, random_state = 504)\n",
    "X_circles, y_circles = make_circles(n_samples=1000, factor=.65, noise = .05)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = figure.add_subplot(1, 2, 2, title='moons')\n",
    "ax.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=ListedColormap(['green', 'magenta']))\n",
    "\n",
    "ax = figure.add_subplot(1, 2, 1, title='circles')\n",
    "ax.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=ListedColormap(['green', 'magenta']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circles\n",
    "\n",
    "Circles are the simplest of the two. Let's see if we can use a simple network.\n",
    "\n",
    "The code below creates one hidden layer and one output layer. The input layer gets created automatically when we give the hidden layer the input_dim attribute.\n",
    "\n",
    "The output layer is a single node with a sigmoid activation, since this is a binary classifier. Another popular choice for binary classifiers is to make two nodes with a softmax activation. There isn't much difference between the two approaches, but softmax is a real staple of modern neural networks.\n",
    "\n",
    "We're using the classic sigmoid activation function because it's the easiest to interpret.\n",
    "\n",
    "Note, the use of the Adam optimizer. Stochastic gradient descent does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.random.set_seed(504)\n",
    "ann1 = tf.keras.Sequential([\n",
    "  layers.Dense(4, input_dim=2, activation=\"sigmoid\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "ann1.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.01), metrics=['accuracy'])\n",
    "ann1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_circles, y_circles, random_state=504)\n",
    "\n",
    "ann1.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train this enough, we can get it down to almost perfect.\n",
    "\n",
    "**Note:** If you call fit() again, it will continue from the already trained weights. This is a way to experiment with the number of epochs run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probabilities(ann1, X_circles, y_circles, colors, post_process=np.round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a network as simple as this, it's easy to get a feel for how the network works by inspecting the output of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_layers = ann1.layers[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an array containing all the layers but the top one. (That is, the hidden layer and the implicit input layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.keras.Sequential(early_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new neural network with the remaining layers. These layers retain their weights and biases, so we can use this to examine how the network works.\n",
    "\n",
    "It could also be used as a _pre-trained_ network to be used as the basis for another ANN. This is of course more relevant with more complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "plot_probabilities(hidden, X_circles, y_circles, colors, ax=axes[0][0])\n",
    "plot_probabilities(hidden, X_circles, y_circles, colors, dim=1, ax=axes[0][1])\n",
    "plot_probabilities(hidden, X_circles, y_circles, colors, dim=2, ax=axes[1][0])\n",
    "plot_probabilities(hidden, X_circles, y_circles, colors, dim=3, ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moons\n",
    "\n",
    "The moons are more complex, so let's try adding another layer (we could also have tried more neurons in the hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(504)\n",
    "ann2 = tf.keras.Sequential([\n",
    "  layers.Dense(4, input_dim=2, activation=\"sigmoid\"),\n",
    "  layers.Dense(4, activation=\"sigmoid\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "ann2.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200 epochs is a lot for these simple problems. Either they hit a plateau or they converge before that. Instead of fine tuning that each time, let's add a callback.\n",
    "\n",
    "The *early stopping* callback is called after each epoch and evaluates if the optimizer is making much progress (by default by looking at the validation loss). In the example, the fitting stops if no progress has been made for the last 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't go well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_moons, y_moons, random_state=504)\n",
    "\n",
    "history = ann2.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.plot(history.history['loss'], label = 'train')\n",
    "plt.plot(history.history['val_loss'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probabilities(ann2, X_moons, y_moons, colors, post_process=np.round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation function has a very small window of operations before it causes vanishing gradients. For this reason, it is not used much except when you specifically want a probability.\n",
    "\n",
    "We'll replace it with ReLU (tanh works, too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann2 = tf.keras.Sequential([\n",
    "  layers.Dense(4, input_dim=2, activation=\"relu\"),\n",
    "  layers.Dense(4, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "ann2.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.01), metrics=['accuracy'])\n",
    "history = ann2.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid), verbose = 0, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.plot(history.history['loss'], label = 'train')\n",
    "plt.plot(history.history['val_loss'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probabilities(ann2, X_moons, y_moons, colors, post_process=np.round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can _normalize_ the data between layers. The batch normalization layer scales the data to a (learned) mean and standard deviation. This helps keep the values in the active region. It is commonly used with sigmoid and tanh activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann2 = tf.keras.Sequential([\n",
    "  layers.Dense(4, input_dim=2, activation=\"sigmoid\"),\n",
    "  layers.BatchNormalization(),\n",
    "  layers.Dense(4, activation=\"sigmoid\"),\n",
    "  layers.BatchNormalization(),\n",
    "  layers.Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "ann2.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.01), metrics=['accuracy'])\n",
    "history=ann2.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.plot(history.history['loss'], label = 'train')\n",
    "plt.plot(history.history['val_loss'], label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probabilities(ann2, X_moons, y_moons, colors, post_process=np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.keras.Sequential(ann2.layers[:-4])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, ax=axes[0][0])\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, dim=1, ax=axes[0][1])\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, dim=2, ax=axes[1][0])\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, dim=3, ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.keras.Sequential(ann2.layers[:-2])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "def log_(x):\n",
    "    return np.log(x + 1)\n",
    "\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, ax=axes[0][0])\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, dim=1, ax=axes[0][1])\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, dim=2, ax=axes[1][0])\n",
    "plot_probabilities(hidden, X_moons, y_moons, colors, dim=3, ax=axes[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
